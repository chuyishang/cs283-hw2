{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a8b4f1",
   "metadata": {},
   "source": [
    "# Assignment 2: Tokenization, Word Embeddings, and Vector Algebra\n",
    "\n",
    "## Introduction\n",
    "In this assignment, we will explore the problem of getting from sequences of characters to wordtypes with different meanings. You will implement widely-used algorithms for tokenizing text and computing context-less word embeddings, and perform analysis on the results of running these algorithms.\n",
    "\n",
    "Learning objectives:\n",
    "* Understand how byte pair encoding works as a method for tokenizing character sequences, and how it differs from simpler tokenization methods like splitting on whitespace.\n",
    "* Gain insights on how to compute and interpret language statistics, including Zipf's laws of word frequencies.\n",
    "* Understand basics of distributional semantics, including basic implementations of word embedding algorithms.\n",
    "* Learn to use vector algebra as a way of analyzing a space of word embeddings.\n",
    "\n",
    "**Notes:**\n",
    "* In your solution, keep all code as-is except where it's explicitly mentioned to implement a function.\n",
    "* Items marked with a star (â˜…) should be answered in a separate report as a text document with the corresponding item number. You will include your final report as a pdf with your submission.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "You will submit a single zip file `submission.zip` to Gradescope containing the following:\n",
    "\n",
    "```\n",
    "hw2.ipynb # your completed jupyter notebook\n",
    "report.pdf # your written report\n",
    "results/ # outputs from jupyter notebook\n",
    "â”œâ”€â”€ solve_analogy.json\n",
    "â”œâ”€â”€ W_en_fr.npy\n",
    "â”œâ”€â”€ W_en_ja.npy\n",
    "â””â”€â”€ ... (and all other generated files)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-cell-markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to set up the helper function for saving your results. This function will create a directory called `results` and save the output of each of your implemented functions as a `.json` file inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup-cell-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 sentences.\n",
      "First 5 sentences: ['The Happy Prince.', 'HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large red ruby glowed on his sword-hilt.', 'He was very much admired indeed.  â€œHe is as beautiful as a weathercock,â€ remarked one of the Town Councillors who wished to gain a reputation for having artistic tastes; â€œonly not quite so useful,â€ he added, fearing lest people should think him unpractical, which he really was not.', 'â€œWhy canâ€™t you be like the Happy Prince?â€ asked a sensible mother of her little boy who was crying for the moon.  â€œThe Happy Prince never dreams of crying for anything.â€', 'â€œI am glad there is some one in the world who is quite happy,â€ muttered a disappointed man as he gazed at the wonderful statue.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.normalizers import Lowercase\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def save_results(results, filename):\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    with open(os.path.join('results', filename), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "# Load the corpus\n",
    "with open('data/stories.txt', 'r', encoding='utf-8') as f:\n",
    "    en_corpus = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(en_corpus)} sentences.\")\n",
    "print(\"First 5 sentences:\", en_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2c1d0",
   "metadata": {},
   "source": [
    "## Part 1: Tokenization (25 points)\n",
    "\n",
    "In this part, you will implement two standard methods for tokenization to convert a sequence of characters into a sequence of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7e6c5",
   "metadata": {},
   "source": [
    "### Part 1.0: Setup\n",
    "**Implement:** A helper function that, given a sequence of tokenized sentences, returns a dictionary pairing word types with their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4aef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_types(tokenized_sentences: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Given a list of tokenized sentences, return a dictionary mapping each word type to its frequency.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    d = defaultdict(int)\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            d[token] += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "word_counts_output = count_word_types(en_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "### Part 1.1: Simple tokenization (10 points)\n",
    "**Implement:** A function that splits a given input string by whitespace into sequences of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "k1l2m3n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits a given input string by whitespace into a sequence of tokens.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f208daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 558753), ('e', 291861), ('t', 208957), ('a', 182766), ('h', 176648), ('o', 166640), ('n', 150590), ('i', 133220), ('s', 126879), ('r', 119415)]\n",
      "2938871\n"
     ]
    }
   ],
   "source": [
    "freq_dict = count_word_types(en_corpus)\n",
    "sorted_freq_dict = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_freq_dict[:10])\n",
    "print(sum(freq_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "â˜… 1.1.1 Use the helper function to create a frequency dictionary for the sentences in `en_corpus`. What are the 10 most frequent wordtypes?\n",
    "\n",
    "â˜… 1.1.2 What are two limitations you observe in this simple tokenization method? Include examples that motivate these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9t0u1v2",
   "metadata": {},
   "source": [
    "### Part 1.2: Byte pair encoding (15 points)\n",
    "**Implement:** The Byte Pair Encoding (BPE) algorithm by completing the methods in the class below.\n",
    "\n",
    "You'll need a few components:\n",
    "1. A function to get pair frequencies from the vocabulary.\n",
    "2. A function to merge a given pair in the vocabulary.\n",
    "3. A main training function that iteratively finds the best pair and merges it.\n",
    "4. A tokenizer function that applies the learned merges to new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5fe632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# I've done the Stanford 336 Course by Percy Liang and Tatsu independently, so I referenced that when doing this.\n",
    "class BPE():\n",
    "    \"\"\"Byte Pair Encoding (BPE) tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the BPE tokenizer.\n",
    "\n",
    "        Attributes:\n",
    "            merges (dict[tuple(str), str]): Dictionary mapping pairs of tuples to their merged symbol.\n",
    "            vocab (dict[int, str]): Dictionary mapping integer ids to their corresponding tokens.\n",
    "        \"\"\"\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "\n",
    "    def get_stats(self, word_freqs: Counter[str]) -> Counter[tuple[str, str]]:\n",
    "        \"\"\"Counts the frequency of pairs in the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            word_freqs: Counter[str]\n",
    "        \n",
    "        Returns:\n",
    "            Counter[tuple[str, str]] representing the frequency of each pair in the vocabulary.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function\n",
    "        pair_counts = Counter()\n",
    "        for word in word_freqs.keys():\n",
    "            word_tokens = word.split()\n",
    "            for i in range(len(word_tokens)-1):\n",
    "                pair_counts[(word_tokens[i], word_tokens[i+1])] += word_freqs[word]\n",
    "        return pair_counts\n",
    "    \n",
    "    def get_most_frequent_pair(self, pair_counts: Counter[tuple[str, str]]) -> tuple[str, str]:\n",
    "        \"\"\"Helper function to get the most frequent pair from the pair counts.\n",
    "\n",
    "        Args:\n",
    "            pair_counts (Counter[tuple[str, str]]): Counter object containing the frequency of each pair in the vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[str, str] representing the most frequent pair in the vocabulary.\n",
    "        \"\"\"\n",
    "        return max(pair_counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def merge(self, pair: tuple[str, str], word_freqs: Counter[str]) -> None:\n",
    "        \"\"\"Merges a pair of symbols into a new symbol in the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            pair (tuple[str, str]): Pair of symbols to merge.\n",
    "            word_freqs (Counter[str]): Counter object containing the frequency of each word in the vocabulary.\n",
    "        \n",
    "        Outputs:\n",
    "            None, but updates the word_freqs counter to reflect the new symbol.\n",
    "        \"\"\"\n",
    "        merged_token = pair[0] + pair[1]\n",
    "        \n",
    "        self.vocab[len(self.vocab)] = merged_token\n",
    "        self.merges[pair] = merged_token\n",
    "        \n",
    "        new_word_freqs = Counter()\n",
    "        for word, freq in word_freqs.items():\n",
    "            tokens = word.split()\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                    new_tokens.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            new_word = ' '.join(new_tokens)\n",
    "            new_word_freqs[new_word] += freq\n",
    "        \n",
    "        word_freqs.clear()\n",
    "        word_freqs.update(new_word_freqs)\n",
    "\n",
    "\n",
    "    def train(self, corpus: list[str], vocab_size: int):\n",
    "        \"\"\"Trains the tokenizer on a given corpus.\n",
    "        \n",
    "        Args:\n",
    "            corpus (list[str]): Corpus containing a list of sentences.\n",
    "            vocab_size (int): Maximum size of the vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            ???\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Initialize vocabulary and pre-tokenize the corpus\n",
    "        initial_vocab = set()\n",
    "        word_freqs = defaultdict(int)\n",
    "        \n",
    "        # Prepare the initial word frequency map\n",
    "        for text in corpus:\n",
    "            words = text.strip().split()\n",
    "            for word in words:\n",
    "                processed_word = ' '.join(list(word))\n",
    "                word_freqs[processed_word] += 1\n",
    "                initial_vocab.update(list(word))\n",
    "\n",
    "        # The initial vocabulary includes all single characters\n",
    "        self.vocab = {i: char for i, char in enumerate(sorted(list(initial_vocab)))}\n",
    "\n",
    "        # 2. Iteratively merge the most frequent pair\n",
    "        # TODO: Implement this function\n",
    "\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            most_frequent_pair = self.get_most_frequent_pair(self.get_stats(word_freqs))\n",
    "            self.merge(most_frequent_pair, word_freqs)\n",
    "        return self.vocab\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Tokenizes a new sentence using the learned merge rules.\"\"\"\n",
    "        # TODO: Implement this function\n",
    "        words = text.split()\n",
    "        tokenized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            tokens = list(word)\n",
    "            \n",
    "            for pair, merged_token in self.merges.items():\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                        new_tokens.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                tokens = new_tokens\n",
    "            \n",
    "            tokenized_words.extend(tokens)\n",
    "        \n",
    "        return tokenized_words\n",
    "\n",
    "    def decode(self, tokens: list[str]) -> str:\n",
    "        \"\"\"Decodes a sequence of tokens back into a string.\"\"\"\n",
    "        # TODO: Implement this function\n",
    "        return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142cb6c0",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save your results for `BPE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f178240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for BPE merges and vocabulary.\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = BPE()\n",
    "bpe_tokenizer.train(en_corpus, vocab_size=500)\n",
    "\n",
    "# Save the learned merges and vocabulary\n",
    "save_results({str(k): v for k, v in bpe_tokenizer.merges.items()}, 'bpe_merges.json')\n",
    "save_results(bpe_tokenizer.vocab, 'bpe_vocab.json')\n",
    "print(\"Saved results for BPE merges and vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "â˜… 1.2.1 What are the 10 most frequent wordtypes from your BPE tokenizer?\n",
    "\n",
    "â˜… 1.2.2 Generate a plot showing the Zipfian distribution of wordtypes (i.e., a log-log plot of frequency vs. rank). Compare the distributions from simple whitespace tokenization and BPE. Include the plots and your comparison in the report, make sure your plot is clearly labeled.\n",
    "\n",
    "â˜… 1.2.3 Why might these distributions look different from one another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bpe_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the corpus using the BPE tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [bpe_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m en_corpus]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Flatten the list of tokenized sentences into a single list of tokens\u001b[39;00m\n\u001b[1;32m      8\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized_corpus \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence]\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize the corpus using the BPE tokenizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tokenized_corpus \u001b[38;5;241m=\u001b[39m [\u001b[43mbpe_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m en_corpus]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Flatten the list of tokenized sentences into a single list of tokens\u001b[39;00m\n\u001b[1;32m      8\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tokenized_corpus \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bpe_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Use this space to generate your Zipfian plot.\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the corpus using the BPE tokenizer\n",
    "tokenized_corpus = [bpe_tokenizer.tokenize(sentence) for sentence in en_corpus]\n",
    "\n",
    "# Flatten the list of tokenized sentences into a single list of tokens\n",
    "all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
    "\n",
    "# Count the frequency of each token\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Get the 10 most common tokens\n",
    "most_common_tokens = token_counts.most_common(10)\n",
    "\n",
    "# Print the 10 most frequent wordtypes\n",
    "print(\"10 most frequent wordtypes from BPE tokenizer:\")\n",
    "for token, count in most_common_tokens:\n",
    "    print(f\"Token: {token}, Frequency: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## Part 2: Word embeddings (25 points)\n",
    "\n",
    "In this part, you will implement the Skip-Gram model to compute word embeddings for the words in your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee44fd8",
   "metadata": {},
   "source": [
    "### Part 2.0: Setup\n",
    "For the remainder of the assignment, we will use `ByteLevelBPETokenizer` from the `tokenizers` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4aee916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_tokenizer(data_path: str, vocab_size=10000):\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.normalizer = Lowercase()\n",
    "\n",
    "    tokenizer.train(files=data_path, vocab_size=vocab_size, min_frequency=2)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "en_tokenizer = get_tokenizer(\"data/stories.txt\")\n",
    "en_tokenized = [en_tokenizer.encode(s).ids for s in en_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1v2w3x4",
   "metadata": {},
   "source": [
    "### Part 2.1: Experiment with different contexts (5 points)\n",
    "\n",
    "First, let's explore different implementations of context for the Skip-Gram model.\n",
    "\n",
    "**Implement:** Two functions that map from tokenized sentences and words to contexts:\n",
    "\n",
    "##### a)  **Bag-of-Words Context** \n",
    "The first function should, for each target token $w$, create a positive example $(w, c)$ for every additional token $c$ in the sentence. \n",
    "\n",
    "I.e., for a sentence $\\langle x_1, \\dots, x_m \\rangle$ of length $m$ tokens, we should create $m \\cdot (m - 1)$ positive examples: $\\bigcup_{1 \\leq i \\leq m} \\bigcup_{1 \\leq j \\leq m, i \\neq j} \\left\\{ (x_i, x_j) \\right\\}$.\n",
    "   \n",
    "##### b)  **Neighboring Tokens Context** \n",
    "\n",
    "The second function should, for each target token $w$, create a positive example $(w, c)$ for each of the tokens within a window of up to size $N$ before and after the target token's index. \n",
    "\n",
    "I.e., for a sentence $\\langle x_1, \\dots, x_m \\rangle$ of length $m$ tokens, we should create $m \\cdot 2N$ positive examples: $\\bigcup_{1 \\leq i \\leq m}\\bigcup_{1 \\leq j \\leq N}\\{ (x_i, (-j, x_{i-j})), (x_i, (j, x_{i + j}) )\\}$. \n",
    "\n",
    "Here, contexts are  not just individual wordtypes, but the wordtypes are paired with some positive or negative distance away from the target token. For cases where the window is out of bounds of the sentence (i.e., $i - j < 1$ or $i + j > m$), you can create a special wordtype (e.g., \\<BOS\\> or \\<EOS\\>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "y5z6a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_context(tokenized_sentences: List[List[str]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Creates positive (target, context) examples where context is any other word in the sentence.\n",
    "    NOTE: this is modified to return ints, as according to the Ed clarifications.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    out_context = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(len(sentence)):\n",
    "                if i != j:\n",
    "                    out_context.append((sentence[i], sentence[j]))\n",
    "    return out_context\n",
    "\n",
    "\n",
    "def get_neighbor_context(tokenized_sentences: List[List[int]], window_size: int) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Creates positive (target, context) examples from tokens within a given window size.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    out_context = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(1, window_size+1):\n",
    "                ctx_left_idx = i - j\n",
    "                ctx_right_idx = i + j\n",
    "                if ctx_left_idx >= 0:\n",
    "                    out_context.append((sentence[i], sentence[ctx_left_idx]))\n",
    "                if ctx_right_idx < len(sentence):\n",
    "                    out_context.append((sentence[i], sentence[ctx_right_idx]))\n",
    "    return out_context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d617db",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save your results for `get_neighbor_context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22017200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results for get_neighbor_context.\n"
     ]
    }
   ],
   "source": [
    "neighbor_pairs = get_neighbor_context(en_tokenized, window_size=2)\n",
    "save_results(neighbor_pairs[:10000], 'neighbor_context.json')\n",
    "print(\"Saved results for get_neighbor_context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "### Part 2.3: Negative sampling (5 points)\n",
    "To train the Skip-Gram model efficiently, we use negative sampling. First, we model the probability distribution over contexts.\n",
    "\n",
    "**Implement:** A function that computes a distribution over the contexts $\\mathcal{C}$ given a dataset of word-context pairs $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba7efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_distribution(pairs: List[Tuple[int, int]], vocab_size: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes the probability distribution over contexts P(c).\n",
    "\n",
    "    Args:\n",
    "        pairs: A list of (token, context) pairs.\n",
    "        vocab_size: The size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        A list of probabilities for each context token, ordered by token index.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    context_distribution = [0] * vocab_size\n",
    "    for pair in pairs:\n",
    "        context_distribution[pair[1]] += 1\n",
    "    context_distribution = [x / len(pairs) for x in context_distribution]\n",
    "    return torch.tensor(context_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c831a9b",
   "metadata": {},
   "source": [
    "**Implement:** A function for sampling negative pairs of tokens and contexts given the prior over contexts $\\mathcal{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c23ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_negative_contexts(context_distribution: torch.Tensor, num_samples: int, batch_size=1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Samples negative contexts based on the context distribution.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    batched_distribution = context_distribution.repeat(batch_size, 1)\n",
    "    return torch.multinomial(batched_distribution, num_samples, replacement=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4-report",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "â˜… 2.3.1 For each of the following context settings, list the highest-probability *negative* sample according to the priors you computed:\n",
    "- Context as a bag of words.\n",
    "- Context as neighboring tokens, $N=1$.\n",
    "- Context as neighboring tokens, $N=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104e13fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      ",\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x744a354902b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shang/miniconda3/envs/ugrad-nlp/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(en_tokenizer\u001b[38;5;241m.\u001b[39mdecode([highest_prob_token]))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# For BOW\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m neighbor_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mget_bow_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_tokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m ctx_dist \u001b[38;5;241m=\u001b[39m get_context_distribution(neighbor_pairs, \u001b[38;5;28mlen\u001b[39m(en_tokenizer\u001b[38;5;241m.\u001b[39mget_vocab()))\n\u001b[1;32m     18\u001b[0m samples \u001b[38;5;241m=\u001b[39m sample_negative_contexts(ctx_dist, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mget_bow_context\u001b[0;34m(tokenized_sentences)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m---> 12\u001b[0m                 \u001b[43mout_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_context\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: Scratch work here\n",
    "neighbor_pairs = get_neighbor_context(en_tokenized, window_size=2)\n",
    "ctx_dist = get_context_distribution(neighbor_pairs, len(en_tokenizer.get_vocab()))\n",
    "samples = sample_negative_contexts(ctx_dist, 10000, 1)\n",
    "highest_prob_token = torch.argmax(ctx_dist)\n",
    "print(en_tokenizer.decode([highest_prob_token]))\n",
    "\n",
    "\n",
    "neighbor_pairs = get_neighbor_context(en_tokenized, window_size=1)\n",
    "ctx_dist = get_context_distribution(neighbor_pairs, len(en_tokenizer.get_vocab()))\n",
    "samples = sample_negative_contexts(ctx_dist, 10000, 1)\n",
    "highest_prob_token = torch.argmax(ctx_dist)\n",
    "print(en_tokenizer.decode([highest_prob_token]))\n",
    "\n",
    "# For BOW\n",
    "neighbor_pairs = get_bow_context(en_tokenized)\n",
    "ctx_dist = get_context_distribution(neighbor_pairs, len(en_tokenizer.get_vocab()))\n",
    "samples = sample_negative_contexts(ctx_dist, 10000, 1)\n",
    "highest_prob_token = torch.argmax(ctx_dist)\n",
    "print(en_tokenizer.decode([highest_prob_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8-analysis",
   "metadata": {},
   "source": [
    "### Part 2.4: Train the Skip-Gram model and analyze embeddings (15 points)\n",
    "\n",
    "Now, we'll define our Skip-Gram model and train it on our corpus.\n",
    "\n",
    "**Implement:** A Skip-Gram model in PyTorch. The model should consist of two embedding layers: one for the target words and one for the context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8347e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # TODO: Implement the model layers\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        # TODO: Implement the forward pass\n",
    "        # targets should be (B, 1)\n",
    "        # context should be (B, K)\n",
    "        target_embeds = self.target_embeddings(target) # (B, 1, D)\n",
    "        context_embeds = self.output_embeddings(context) # (B, K, D)\n",
    "        return torch.sum(target_embeds * context_embeds, dim=-1) # (B, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e485f4",
   "metadata": {},
   "source": [
    "**Implement:** Train your Skip-Gram model using the tokenized data `en_tokenized`. You may want to implement batching to speed things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda'\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[int, int]]):\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "def train_skipgram(corpus, tokenizer, num_epochs=30):\n",
    "\n",
    "    # Initialize the model, loss function, optimizer, and dataloader\n",
    "\n",
    "    embedding_dim = 128\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    batch_size = 2**16\n",
    "\n",
    "    neighbor_pairs = get_neighbor_context([tokenizer.encode(s).ids for s in corpus], window_size=5)\n",
    "    dataset = SkipGramDataset(neighbor_pairs)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )    \n",
    "\n",
    "    # Training loop\n",
    "    # TODO: Implement this. \n",
    "    # You will need to combine positive and negative samples in the loss computation, and adjust the labels accordingly.\n",
    "    NUM_NEGATIVE_SAMPLES = 3\n",
    "    context_dist = get_context_distribution(neighbor_pairs, vocab_size)\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in enumerate(tqdm(dataloader)):\n",
    "            target, positive_context = batch\n",
    "            negative_contexts = sample_negative_contexts(context_dist, NUM_NEGATIVE_SAMPLES, target.shape[0]) # (B, K-1)\n",
    "            target = target.unsqueeze(1).to(device) # (B, 1)\n",
    "            positive_context = positive_context.unsqueeze(1) # (B, 1)\n",
    "            full_context = torch.cat([positive_context, negative_contexts], dim=-1).to(device) # (B, K)\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                scores = model(target, full_context) # (B, K)\n",
    "                # labels = torch.zeros_like(target.shape[0], NUM_NEGATIVE_SAMPLES + 1)  # (B, K)\n",
    "                # labels[:, 0] = torch.ones(target.shape[0])\n",
    "                labels = torch.zeros_like(scores, dtype=torch.float, device=device)\n",
    "                labels[:, 0] = 1.0\n",
    "                labels = labels.to(device)\n",
    "                loss = criterion(scores, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Loss: {loss.item()}\")\n",
    "        print(f\"EPOCH {epoch+1} COMPLETED\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a23331d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TIMING\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx): return self.pairs[idx]\n",
    "\n",
    "def _cuda_timer():\n",
    "    \"\"\"Returns (start_event, end_event, measure_fn) where measure_fn() -> milliseconds\"\"\"\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end   = torch.cuda.Event(enable_timing=True)\n",
    "    def measure():\n",
    "        torch.cuda.synchronize()\n",
    "        return start.elapsed_time(end)  # ms\n",
    "    return start, end, measure\n",
    "\n",
    "def train_skipgram(corpus, tokenizer, num_epochs=30):\n",
    "\n",
    "    embedding_dim = 128\n",
    "    vocab_size = len(tokenizer.get_vocab())\n",
    "    model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    batch_size = 2**16\n",
    "\n",
    "    neighbor_pairs = get_neighbor_context([tokenizer.encode(s).ids for s in corpus], window_size=5)\n",
    "    dataset = SkipGramDataset(neighbor_pairs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    NUM_NEGATIVE_SAMPLES = 3\n",
    "    context_dist = get_context_distribution(neighbor_pairs, vocab_size)\n",
    "    probs_cuda = torch.as_tensor(context_dist, device=device, dtype=torch.float)\n",
    "    probs_cuda = probs_cuda / probs_cuda.sum().clamp_min(1e-12)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ---- Profiling accumulators (milliseconds) ----\n",
    "    acc = defaultdict(float)\n",
    "    steps_counted = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"epoch {epoch+1}\")\n",
    "        for step, batch in enumerate(pbar):\n",
    "            # Timers\n",
    "            ns_s, ns_e, ns_ms = _cuda_timer()\n",
    "            move_s, move_e, move_ms = _cuda_timer()\n",
    "            fwd_s, fwd_e, fwd_ms = _cuda_timer()\n",
    "            loss_s, loss_e, loss_ms = _cuda_timer()\n",
    "            bwd_s, bwd_e, bwd_ms = _cuda_timer()\n",
    "            opt_s, opt_e, opt_ms = _cuda_timer()\n",
    "\n",
    "            # ---- Negative sampling (likely CPU op unless your sampler uses CUDA) ----\n",
    "            # If your sample_negative_contexts runs on CPU, we time with wall clock.\n",
    "            t0 = time.time()\n",
    "            # negative_contexts = sample_negative_contexts(context_dist, NUM_NEGATIVE_SAMPLES, batch[0].shape[0]).to(device, non_blocking=True)  # (B, K-1)\n",
    "            B = batch_size\n",
    "            negative_contexts = torch.multinomial(\n",
    "                probs_cuda.expand(B, -1), NUM_NEGATIVE_SAMPLES, replacement=True\n",
    "            )  # (B, Kneg) on GPU already\n",
    "\n",
    "            t1 = time.time()\n",
    "            acc[\"neg_sampling_wall_ms\"] += (t1 - t0) * 1000.0\n",
    "\n",
    "            # ---- Move + concat ----\n",
    "            target, positive_context = batch\n",
    "            ns_s.record()  # (anchor to show order; not used if CPU)\n",
    "            ns_e.record()\n",
    "\n",
    "            move_s.record()\n",
    "            target = target.unsqueeze(1).to(device, non_blocking=True)                     # (B,1)\n",
    "            positive_context = positive_context.unsqueeze(1).to(device, non_blocking=True) # (B,1)\n",
    "            # If negatives are CPU LongTensor, concat on CPU then move once is sometimes faster for huge B:\n",
    "            # full_context = torch.cat([positive_context.cpu(), negative_contexts], dim=-1).to(device, non_blocking=True)\n",
    "            full_context = torch.cat([positive_context, negative_contexts], dim=-1).to(device, non_blocking=True)  # (B,K)\n",
    "            move_e.record()\n",
    "\n",
    "            # ---- Forward ----\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                fwd_s.record()\n",
    "                scores = model(target, full_context)  # (B,K)\n",
    "                fwd_e.record()\n",
    "\n",
    "                # ---- Loss (cast logits to fp32 for numerical stability) ----\n",
    "                loss_s.record()\n",
    "                labels = torch.zeros_like(scores, dtype=torch.float, device=device)\n",
    "                labels[:, 0] = 1.0\n",
    "                loss = criterion(scores.float(), labels)\n",
    "                loss_e.record()\n",
    "\n",
    "            # ---- Backward ----\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            bwd_s.record()\n",
    "            loss.backward()\n",
    "            bwd_e.record()\n",
    "\n",
    "            # ---- Optimizer ----\n",
    "            opt_s.record()\n",
    "            optimizer.step()\n",
    "            opt_e.record()\n",
    "\n",
    "            # Ensure events completed before reading times\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            # Accumulate GPU times\n",
    "            acc[\"move_ms\"] += move_ms()\n",
    "            acc[\"forward_ms\"] += fwd_ms()\n",
    "            acc[\"loss_ms\"] += loss_ms()\n",
    "            acc[\"backward_ms\"] += bwd_ms()\n",
    "            acc[\"optimizer_ms\"] += opt_ms()\n",
    "            acc[\"step_total_ms\"] += (move_ms() + fwd_ms() + loss_ms() + bwd_ms() + opt_ms())\n",
    "            steps_counted += 1\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "        # ---- Per-epoch summary ----\n",
    "        denom = max(steps_counted, 1)\n",
    "        print(\n",
    "            f\"[epoch {epoch+1}] avg per step (ms): \"\n",
    "            f\"neg_sampling_wall={acc['neg_sampling_wall_ms']/denom:.2f}, \"\n",
    "            f\"move={acc['move_ms']/denom:.2f}, \"\n",
    "            f\"forward={acc['forward_ms']/denom:.2f}, \"\n",
    "            f\"loss={acc['loss_ms']/denom:.2f}, \"\n",
    "            f\"backward={acc['backward_ms']/denom:.2f}, \"\n",
    "            f\"optimizer={acc['optimizer_ms']/denom:.2f}, \"\n",
    "            f\"GPU_totalâ‰ˆ{acc['step_total_ms']/denom:.2f}\"\n",
    "        )\n",
    "        # reset per-epoch\n",
    "        acc.clear(); steps_counted = 0\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72e2b2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:   0%|          | 0/101 [00:00<?, ?it/s]/tmp/ipykernel_478767/3214858600.py:88: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "epoch 1:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 100/101 [00:32<00:00,  3.09it/s, loss=1.98]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 37776 but got size 65536 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_skipgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_center_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/center_word.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m test_context_words \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/context_words.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[24], line 84\u001b[0m, in \u001b[0;36mtrain_skipgram\u001b[0;34m(corpus, tokenizer, num_epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m positive_context \u001b[38;5;241m=\u001b[39m positive_context\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# (B,1)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# If negatives are CPU LongTensor, concat on CPU then move once is sometimes faster for huge B:\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# full_context = torch.cat([positive_context.cpu(), negative_contexts], dim=-1).to(device, non_blocking=True)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m full_context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpositive_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_contexts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (B,K)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m move_e\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# ---- Forward ----\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 37776 but got size 65536 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = train_skipgram(en_corpus, en_tokenizer, num_epochs=2)\n",
    "\n",
    "test_center_word = torch.tensor(np.load('data/center_word.npy')).to(device)\n",
    "test_context_words = torch.tensor(np.load('data/context_words.npy')).to(device)\n",
    "\n",
    "test_scores = model(test_center_word, test_context_words)\n",
    "np.save('results/skipgram_scores.npy', test_scores.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855500d3",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save your model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "s9t0u1v2-train",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:01<03:10,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.520859718322754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:35<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2915573120117188\n",
      "EPOCH 1 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:01<03:01,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2473886013031006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:35<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8218541145324707\n",
      "EPOCH 2 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:02<03:46,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.802511990070343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:35<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.681367814540863\n",
      "EPOCH 3 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:02<03:40,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6666335463523865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:34<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6186714172363281\n",
      "EPOCH 4 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:02<03:33,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6099650263786316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:34<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5896440148353577\n",
      "EPOCH 5 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:01<02:59,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5785741806030273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:32<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5713170170783997\n",
      "EPOCH 6 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:02<03:42,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.562330961227417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [02:31<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5618919134140015\n",
      "EPOCH 7 COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:02<03:31,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5517832636833191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 80/101 [02:01<00:31,  1.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_skipgram\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_center_word \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/center_word.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m test_context_words \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/context_words.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mtrain_skipgram\u001b[0;34m(corpus, tokenizer, num_epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader)):\n\u001b[1;32m     42\u001b[0m     target, positive_context \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 43\u001b[0m     negative_contexts \u001b[38;5;241m=\u001b[39m \u001b[43msample_negative_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_NEGATIVE_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, K-1)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     positive_context \u001b[38;5;241m=\u001b[39m positive_context\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, 1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36msample_negative_contexts\u001b[0;34m(context_distribution, num_samples, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: Implement this function\u001b[39;00m\n\u001b[1;32m      6\u001b[0m batched_distribution \u001b[38;5;241m=\u001b[39m context_distribution\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = train_skipgram(en_corpus, en_tokenizer, num_epochs=30)\n",
    "\n",
    "test_center_word = torch.tensor(np.load('data/center_word.npy')).to(device)\n",
    "test_context_words = torch.tensor(np.load('data/context_words.npy')).to(device)\n",
    "\n",
    "test_scores = model(test_center_word, test_context_words)\n",
    "np.save('results/skipgram_scores.npy', test_scores.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130d444",
   "metadata": {},
   "source": [
    "**Implement:** A function that, given a trained Skip-Gram model and a target wordtype, returns the cosine similarity of that wordtype's embedding with the embedding of every other wordtype in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(model: SkipGramModel, tokenizer: ByteLevelBPETokenizer, target_word: str) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Given a trained model and a target word, returns a list of (word, similarity_score) \n",
    "    for all words in the vocabulary (excluding the target word), sorted by similarity score.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    embedding_matrix = model.target_embeddings.weight.detach() # (V, D)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    target_id = vocab[target_word]\n",
    "    target_embed = embedding_matrix[target_id] # (D, )\n",
    "    sims = (embedding_matrix @ target_embed.unsqueeze(1)).flatten() # (V)\n",
    "    out_list = []\n",
    "    for i, sim in enumerate(sims):\n",
    "        if i != target_id:\n",
    "            out_list.append((tokenizer.decode([i]), sim))\n",
    "    out_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40e850",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "â˜… 2.4.1 Use `get_cosine_similarity` to explore similar wordtypes for different English words. Give three examples of pairs of similar wordtypes that make sense, and three that don't. What are possible reasons for a high similarity score between two seemingly unrelated words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-part3",
   "metadata": {},
   "source": [
    "## Part 3: Vector algebra (25 points)\n",
    "\n",
    "For the remainder of this homework, you will use a modern NLP library `spaCy`. You can find its documentation [here](https://spacy.io/usage/spacy-101).\n",
    "\n",
    "Install spaCy and its English, French, Japanese models with\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download fr_core_news_md\n",
    "python -m spacy download ja_core_news_md\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d948c",
   "metadata": {},
   "source": [
    "### Part 3.0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_md\")\n",
    "nlp_ja = spacy.load(\"ja_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8-analogy",
   "metadata": {},
   "source": [
    "### Part 3.1: Word analogy (10 points)\n",
    "\n",
    "**Implement:** A function that, given two wordtypes representing a target analogy (e.g., *cat* and *kitten*) and an input wordtype to evaluate in that analogy (e.g., *dog*), returns the wordtype that (according to the learned embeddings) best fits as the analogy. Recall the formula for finding this in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2-analogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(model, a: str, b: str, c: str, n=5) -> str:\n",
    "    \"\"\"\n",
    "    Solves the analogy 'a is to b as c is to ?'.\n",
    "    Returns the word from the vocabulary whose embedding is closest to vec(b) - vec(a) + vec(c).\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f32a1",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save your results for `solve_analogy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_analogies = [\n",
    "    (\"king\", \"queen\", \"man\", \"woman\"),\n",
    "    (\"paris\", \"france\", \"berlin\", \"germany\"),\n",
    "    (\"car\", \"driver\", \"train\", \"conductor\"),\n",
    "    (\"good\", \"better\", \"bad\", \"worse\"),\n",
    "    (\"big\", \"bigger\", \"small\", \"smaller\"),\n",
    "    (\"dog\", \"puppy\", \"cat\", \"kitty\"),\n",
    "]\n",
    "\n",
    "\n",
    "analogy_results = []\n",
    "for a, b, c, expected in test_analogies:\n",
    "    result = solve_analogy(nlp_en, a, b, c)\n",
    "    analogy_results.append(result)\n",
    "    print(f\"{a}:{b}::{c}:? -> {result} (expected: {expected})\")\n",
    "\n",
    "save_results(analogy_results, 'solve_analogy.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6-report",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "â˜… 3.1.1 Find three analogies in English that the model is able to solve correctly, and three analogies in English where the model gives a wrong answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226556e0",
   "metadata": {},
   "source": [
    "### 3.2 Embedding alignment (15 points)\n",
    "\n",
    "**Implement:** A function that identifies the transformation $W$ that best aligns two embedding spaces given a parallel corpus and a parallel word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "def learn_translation_matrix(\n",
    "    nlp_src: Language,\n",
    "    nlp_tgt: Language,\n",
    "    word_pairs: List[List[str]]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Learns a linear transformation matrix W that maps source language vectors to target language vectors.\n",
    "    Input:\n",
    "        nlp_src: spaCy Language object for the source language.\n",
    "        nlp_tgt: spaCy Language object for the target language.\n",
    "        word_pairs: A list of [source_word, target_word] pairs.\n",
    "    Output:\n",
    "        W: A numpy array of shape (d, d) where d is the embedding dimension\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Create the training matrices X (Source) and Y (Target)\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    for src_word, tgt_word in word_pairs:\n",
    "        if nlp_src.vocab[src_word].has_vector and nlp_tgt.vocab[tgt_word].has_vector:\n",
    "            # Normalize vectors to focus on direction\n",
    "            src_vector = nlp_src.vocab[src_word].vector\n",
    "            tgt_vector = nlp_tgt.vocab[tgt_word].vector\n",
    "            X_list.append(src_vector / np.linalg.norm(src_vector))\n",
    "            Y_list.append(tgt_vector / np.linalg.norm(tgt_vector))\n",
    "\n",
    "    X = np.array(X_list)\n",
    "    Y = np.array(Y_list)\n",
    "    \n",
    "    # 2. Learn the transformation matrix W where WX â‰ˆ Y\n",
    "    # TODO: Implement this\n",
    "    pass\n",
    "\n",
    "def translate_with_matrix(\n",
    "    nlp_src: Language, nlp_tgt: Language, W: np.ndarray, word: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Translates a word from the source language to the target language using the learned matrix W.\n",
    "    Input:\n",
    "        nlp_src: spaCy Language object for the source language.\n",
    "        nlp_tgt: spaCy Language object for the target language.\n",
    "        W: The learned transformation matrix.\n",
    "        word: The source language word to translate.\n",
    "    Output:\n",
    "        predicted_word: The translated word in the target language.\n",
    "    \"\"\"\n",
    "    src_vec = nlp_src.vocab[word].vector\n",
    "    src_vec_norm = src_vec / np.linalg.norm(src_vec)\n",
    "\n",
    "    tgt_vec_pred = W @ src_vec_norm\n",
    "    tgt_vec_pred_norm = tgt_vec_pred / np.linalg.norm(tgt_vec_pred)\n",
    "\n",
    "    all_tgt_vectors = nlp_tgt.vocab.vectors.data\n",
    "    all_tgt_vectors_norm = all_tgt_vectors / np.linalg.norm(all_tgt_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    similarities = np.dot(all_tgt_vectors_norm, tgt_vec_pred_norm)\n",
    "    best_match_index = np.argmax(similarities)\n",
    "\n",
    "    hash_value = list(nlp_tgt.vocab.vectors.keys())[best_match_index]\n",
    "    predicted_word = nlp_tgt.vocab.strings[hash_value]\n",
    "    return predicted_word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a94ca",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save your results for `learn_translation_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39197d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ja = json.load(open('data/en-ja.json'))\n",
    "en_fr = json.load(open('data/en-fr.json'))\n",
    "\n",
    "W_en_fr = learn_translation_matrix(nlp_en, nlp_fr, en_fr)\n",
    "W_en_ja = learn_translation_matrix(nlp_en, nlp_ja, en_ja)\n",
    "\n",
    "# Save your matrices\n",
    "np.save('results/W_en_fr.npy', W_en_fr)\n",
    "np.save('results/W_en_ja.npy', W_en_ja)\n",
    "print(\"Saved translation matrices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f16306",
   "metadata": {},
   "source": [
    "**Report:** Generate transformation matrices to translate between English<->Japanese and English<->French.\n",
    "\n",
    "â˜… 3.2.1 What happens when you translate each of these English words to Japanese/French, then back to English?\n",
    "\n",
    "1. penguin\n",
    "2. cheese\n",
    "3. sofa\n",
    "4. jacket\n",
    "5. website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b83d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = [\n",
    "    \"penguin\",\n",
    "    \"cheese\",\n",
    "    \"sofa\",\n",
    "    \"jacket\",\n",
    "    \"website\"\n",
    "]\n",
    "\n",
    "# TODO: Use this space to translate the words above using your learned matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0-part4",
   "metadata": {},
   "source": [
    "## Part 4: Computational linguistics (25 points)\n",
    "\n",
    "In this part, you will use `spaCy` to perform linguistic analysis on our corpus. \n",
    "\n",
    "Use the English model from spaCy `nlp_en` to:\n",
    "\n",
    "(a) tokenize\n",
    "(b) lemmatize\n",
    "(c) get dependency trees \n",
    "\n",
    "the first 1000 sentences in our corpus `en_corpus`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a1b2-report",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "A *lemma* is the canonical or dictionary form of a word, representing the base from which all its inflected forms are derived. In NLP, the process of lemmatization groups these various forms (e.g., ran, runs, running) under their single shared lemma (run) to normalize text and simplify analysis.\n",
    "\n",
    "â˜… 4.1.1 Report the 10 most frequent verb lemmas in the corpus.\n",
    "\n",
    "â˜… 4.1.2 For the top 5 most frequent verb lemmas, show the distribution of their subject lemmas (i.e. by navigating the dependency tree from the root to find the `nsubj` item).\n",
    "\n",
    "â˜… 4.1.3 Based on your findings, do you observe any relationship between the types of verbs and the types of subjects they take (e.g., are certain verbs more likely to have animate subjects like 'man' or 'I', versus inanimate subjects like 'door' or 'it')? Discuss one interesting example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4-spacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus_mini = en_corpus[:1000]\n",
    "\n",
    "# TODO: Use spaCy to answer the following questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-bonus",
   "metadata": {},
   "source": [
    "## Part 5: Bonus: Language puzzles (10 points)\n",
    "\n",
    "The following sentences are from a perturbed version of a real language, presented alongside their translations into English. Your task is to match each phrase in **Language Y** with its correct **English** translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0-puzzle",
   "metadata": {},
   "source": [
    "|   | Language Y                     |   | English                  |\n",
    "|---|--------------------------------|---|--------------------------|\n",
    "| A | he shege ptauege yaijeech      | 1 | the cousin's teacher     |\n",
    "| B | heau shege yaijeege stauyau    | 2 | the nurses' cousin       |\n",
    "| C | heau shai ugheedai hyoogeshyau | 3 | the architects' teachers |\n",
    "| D | heau shege eegeege oogheedyau  | 4 | the schools' architect   |\n",
    "| E | he shai stauai eegeech         | 5 | the teachers' schools    |\n",
    "| F | he shai yausai stauich         | 6 | the managers' tools      |\n",
    "| G | he shege hyoogesheege yauseech | 7 | the manager's nurses     |\n",
    "| H | heau shege staueege ptauyau    | 8 | the professor's tool     |\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "â˜… Provide your final matching (e.g., A-1, B-2, C-3, ...). Briefly explain the logic you used to decipher the language, including how possession (e.g., 's) and plurals are marked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4-integrity",
   "metadata": {},
   "source": [
    "## Academic integrity\n",
    "\n",
    "You can discuss high-level solutions with classmates, but your submitted work should be your own. Don't copy from or share code with one another. Provide attribution for any sources of assistance you used in the assignment. If you use generative AI tools to aid in any parts of this assignment, describe how you used them and what worked (or didn't work) when using them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e76859",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugrad-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
